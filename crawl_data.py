import requests
from bs4 import BeautifulSoup
from docx import Document
import os
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# =================== Cáº¤U HÃŒNH ===================
output_dir = 'D:/crawl'
os.makedirs(output_dir, exist_ok=True)
output_path = os.path.join(output_dir, 'thu_tuc_hanh_chinh.docx')
TOTAL_PAGES = 9
MAX_THREADS = 6
ERROR_LOG = os.path.join(output_dir, 'error_urls.txt')

# =================== SESSION Vá»šI RETRY ===================
def create_retry_session():
    session = requests.Session()
    retry = Retry(
        total=5,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
        raise_on_status=False
    )
    adapter = HTTPAdapter(max_retries=retry)
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    return session

session = create_retry_session()

# =================== TÃCH Bá» Cá»¤C ===================
def format_detail_text(text):
    import re
    headers = [
        "MÃ£ thá»§ tá»¥c", "TÃªn thá»§ tá»¥c", "Cáº¥p thá»±c hiá»‡n", "LÄ©nh vá»±c", "ThÃ´ng tin cÃ´ng bá»‘",
        "CÃ¡ch thá»©c ná»™p trá»±c tuyáº¿n", "Thá»i háº¡n giáº£i quyáº¿t", "Má»©c trá»±c tuyáº¿n", "Lá»‡ phÃ­",
        "PhÃ­", "CÆ¡ quan thá»±c hiá»‡n", "Äá»‘i tÆ°á»£ng thá»±c hiá»‡n", "CÃ¡ch thá»©c thá»±c hiá»‡n",
        "Äiá»u kiá»‡n thá»±c hiá»‡n", "Sá»‘ bá»™ há»“ sÆ¡", "Káº¿t quáº£ thá»±c hiá»‡n", "Äá»‹a chá»‰ tiáº¿p nháº­n há»“ sÆ¡",
        "TrÃ¬nh tá»± thá»±c hiá»‡n", "ThÃ nh pháº§n há»“ sÆ¡", "CÄƒn cá»© phÃ¡p lÃ½", "TÃ¬nh tráº¡ng hiá»‡u lá»±c"
    ]
    output = ""
    for header in headers:
        pattern = rf"({header})(.*?)(?=" + "|".join(headers) + "|$)"
        match = re.search(pattern, text, re.DOTALL)
        if match:
            title = match.group(1).strip()
            content = match.group(2).strip()
            output += f"\n\nğŸ”¹ **{title}**\n{content}"
    return output.strip()

# =================== Láº¤Y CHI TIáº¾T Tá»ª LINK ===================
def get_thu_tuc_detail(url):
    try:
        print(f"Äang láº¥y chi tiáº¿t: {url}")
        response = session.get(url, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')

            title_tag = soup.find('div', class_='box-tthc').find('h2', class_='tieude-h2')
            title = title_tag.get_text(strip=True) if title_tag else "KhÃ´ng cÃ³ tiÃªu Ä‘á»"

            detail_div = soup.find('div', class_='table-reponsive')
            if detail_div:
                raw_text = detail_div.get_text(separator=' ', strip=True)
                formatted_detail = format_detail_text(raw_text)

                #  TÃ¬m link file Word
                word_links = []
                for a in detail_div.find_all('a', href=True):
                    href = a['href']
                    if '.doc' in href or '.docx' in href or 'download_file.jsp' in href:
                        file_url = href if href.startswith('http') else 'https://csdl.dichvucong.gov.vn' + href
                        file_name = a.get_text(strip=True)
                        word_links.append(f"{file_name}: {file_url}")

                # ThÃªm pháº§n tÃ i liá»‡u náº¿u cÃ³
                if word_links:
                    formatted_detail += "\n\n **TÃ i liá»‡u Ä‘Ã­nh kÃ¨m**"
                    for link in word_links:
                        formatted_detail += f"\nğŸ“ {link}"

                return {
                    'title': title,
                    'content': formatted_detail
                }
            else:
                return {
                    'title': title,
                    'content': "KhÃ´ng tÃ¬m tháº¥y ná»™i dung chi tiáº¿t."
                }
        else:
            print(f"Lá»—i mÃ£ tráº¡ng thÃ¡i: {response.status_code}")
    except Exception as e:
        print(f"Lá»—i truy cáº­p {url}: {e}")
        with open(ERROR_LOG, "a", encoding="utf-8") as f:
            f.write(url + "\n")
    return None

# =================== CRAWL DANH SÃCH LINK ===================
def crawl_detail_links(links):
    results = []
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        futures = [executor.submit(get_thu_tuc_detail, link) for link in links]
        for future in as_completed(futures):
            detail = future.result()
            if detail:
                results.append(detail)
            time.sleep(random.uniform(0.5, 1.0))  # Giáº£m táº£i server
    return results

# =================== CRAWL Tá»ª TRANG ===================
def crawl_page(page_number):
    print(f"\nÄang crawl trang {page_number}...")
    url = f"https://dichvucong.danang.gov.vn/thu-tuc-hanh-chinh?p_p_id=thutuchanhchinh_WAR_dngdvcportlet&p_p_lifecycle=0&p_p_state=normal&p_p_mode=view&p_p_col_id=column-2&p_p_col_count=1&_thutuchanhchinh_WAR_dngdvcportlet_delta=20&_thutuchanhchinh_WAR_dngdvcportlet_cur={page_number}"
    
    try:
        response = session.get(url, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            tds = soup.find_all('td', class_='table-cell dvc-tendvc')
            links = []

            for td in tds:
                a_tag = td.find('a', href=True)
                if a_tag:
                    href = a_tag['href']
                    if href.startswith('/'):
                        href = 'https://dichvucong.danang.gov.vn' + href
                    if 'thu-tuc-hanh-chinh' in href:
                        links.append(href)

            print(f"âœ… Trang {page_number}: TÃ¬m tháº¥y {len(links)} link")
            return crawl_detail_links(links)
        else:
            print(f"KhÃ´ng táº£i Ä‘Æ°á»£c trang {page_number}: {response.status_code}")
    except Exception as e:
        print(f"Lá»—i khi truy cáº­p trang {page_number}: {e}")
    return []

# =================== GHI RA FILE WORD ===================
def save_all_to_word(details):
    doc = Document()
    for i, item in enumerate(details, 1):
        title = item['title'].strip()
        content = item['content']

        doc.add_heading(f"THá»¦ Tá»¤C {i}: {title}", level=1)

        if content.startswith("ğŸ”¹ **CÆ¡ quan thá»±c hiá»‡n**"):
            parts = content.split("\n\n")
            if len(parts) > 1:
                parts = parts[1:] + [parts[0]]
                content = "\n\n".join(parts)

        for section in content.split("\n\n"):
            para = doc.add_paragraph()
            para.add_run(section.strip()).bold = "ğŸ”¹ **" in section or "ğŸ”¸ **" in section

        doc.add_paragraph("\n")

    doc.save(output_path)
    print(f"ÄÃ£ lÆ°u vÃ o file: {output_path}")

# =================== CHáº Y CHÃNH ===================
def main():
    all_details = []
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        futures = [executor.submit(crawl_page, page) for page in range(1, TOTAL_PAGES + 1)]
        for future in as_completed(futures):
            result = future.result()
            if result:
                all_details.extend(result)
    save_all_to_word(all_details)

if __name__ == "__main__":
    main()
